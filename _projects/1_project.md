---
layout: page
title: Fraud detection model pipeline
description: Building a Machine Learning Pipeline for Fraud Detection
img:
importance: 1
category: fun
toc:
  sidebar: left
---

<div class="col-sm mt-3 mt-md-0">
    {% include figure.html path="assets/img/12.jpg" title="credit card fraud image" class="img-fluid rounded z-depth-1" style="max-width: 50%;"%}
</div>


## Introduction

This project develops a flexible machine learning pipeline tailored for detecting credit card fraud. Its core feature is the ability to train various models through configurable YAML files, allowing users to test different approaches easily. Integrated with MLflow, the pipeline tracks and manages training experiments, aiding in the comparison and evaluation of diverse models.

A significant aspect of this pipeline is its capability to handle imbalanced data, a common challenge in fraud detection. Users can experiment with different sampling methods to balance the dataset, ensuring more accurate and reliable model performance. This flexibility, combined with comprehensive experiment tracking, positions the pipeline as a versatile tool in the fight against credit card fraud.

## Project setup
The project uses a variety of tools and libraries, including 
- pandas for data manipulation, 
- scikit-learn for machine learning, 
- MLflow for experiment tracking,
- joblib for model serialization

The configuration is managed using YAML files, offering a flexible and dynamic way to handle various parameters.

Here is an example of the config yaml file...
```yaml
experiment_name: "RandomForestClassifier hp tuning"
data_path: "data/creditcard.csv"
target: "Class"

model:
  class: "RandomForestClassifier"
  module: "sklearn.ensemble"
  params:
    n_estimators: [100, 500]
    max_depth: [10, 50]
  save_path: "artifacts/model/rf_model.joblib"

preprocessing:
  sampler:
    class: "RandomOverSampler"
    module: "imblearn.over_sampling"
  scaler:
    class: "StandardScaler"
    module: "sklearn.preprocessing"
```
Experimenter can simply change preprocessing and modeling methods by changing the corresponding methods in the config file and run the experiment.

### Imbalance in Dataset: Strategies and Considerations


<div class="col-sm-5 mt-3 mt-md-0" style="float: left; margin-right: 20px;" >
    {% include figure.html path="assets/img/imbalance.jpg" title="example image" class="img-fluid rounded z-depth-1" %}
</div>

In machine learning, especially in problems like credit card fraud detection, class imbalance poses a significant challenge. The imbalance occurs when one class (e.g., fraudulent transactions) is vastly outnumbered by another (e.g., legitimate transactions). This imbalance can lead to models that are biased towards the majority class, often at the expense of poorly predicting the minority class, which is usually the class of interest.

##### Strategies for Handling Imbalance

- **Class Weights**: For models that support the class_weight parameter (such as many in scikit-learn), assigning a higher weight to the minority class helps counterbalance its underrepresentation. In our pipeline, we've integrated `class_weight='balanced'` for such models. This approach effectively tells the model to "pay more attention" to the minority class, which is crucial in fraud detection where failing to detect fraud (a false negative) can be very costly.

- **Sampling Methods**: When class weights are not an option, or to explore different approaches, we can use sampling methods. These methods adjust the dataset before training the model. We've provided options for:
    - **Undersampling**: Reducing the number of instances in the majority class.
    - **Oversampling**: Increasing the number of instances in the minority class.
    - **SMOTE (Synthetic Minority Over-sampling Technique)**: Creating synthetic instances of the minority class.

##### The Debate Around SMOTE

While SMOTE is a popular and powerful method to handle imbalanced datasets, it's not without its critics. Some concerns include:

- **Distorted Data Space**: Synthetic points generated by SMOTE may not represent the true data distribution, potentially leading the model to learn incorrect patterns.

- **Noise**: If the minority class has outliers, SMOTE may amplify these outliers by creating more synthetic instances around them.

### Results and Comparisons

#### Precision-Recall Curves
<div class="col-sm-8 mt-3 mt-md-0" style="float: left; margin-right: 20px;" >
    {% include figure.html path="assets/img/precision-recall-curve.jpg" title="example image" class="img-fluid rounded z-depth-1" %}
</div>

I compared the model performances using precision-recall curve. ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.

##### ROC Curves and Balanced Datasets

  - ROC Curve Basics:
      - The ROC curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.
      - TPR (also known as recall) is calculated as TP / (TP + FN) and FPR is FP / (FP + TN), where TP, FP, FN, and TN represent true positives, false positives, false negatives, and true negatives, respectively.

  - Effect of Balanced Data:
      - In a balanced dataset, where the number of instances in each class is roughly equal, both the TPR and FPR are affected equally by the class distribution.
      - This balance ensures that the ROC curve provides a reliable indication of how well the classifier can distinguish between the two classes under different thresholds.

##### Precision-Recall Curves and Imbalanced Datasets

  - Precision-Recall Curve Basics:
      - The precision-recall curve plots precision (TP / (TP + FP)) against recall (TPR).
      Unlike the ROC curve, the precision-recall curve focuses on the performance of the classifier only on the positive class.

  - Effect of Imbalanced Data:
      - In imbalanced datasets, especially when the positive class (minority class) is much smaller than the negative class, ROC curves can be misleadingly optimistic. This is because the FPR does not capture the model's ability to correctly identify the minority classâ€”it can have a low FPR by simply classifying most instances as the majority class.
      - Precision, however, directly reflects the model's ability to correctly identify positive instances among all instances it labeled as positive. In imbalanced scenarios, this is a more crucial measure.
      - The precision-recall curve is more informative in these situations as it illustrates the trade-off between precision and recall (sensitivity to the minority class) without being affected by the large number of true negatives, which can dominate the FPR measure.

Precision

  - Definition: Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. In the context of fraud detection, it answers the question, "Of all transactions classified as fraudulent, how many are actually fraudulent?"
  - Formula: Precision = TP / (TP + FP), where TP is True Positives and FP is False Positives.
  - Importance: High precision indicates a low rate of false positives (legitimate transactions incorrectly labeled as fraud). This is important in fraud detection because false positives can lead to customer dissatisfaction and unnecessary verification procedures.

Recall (Sensitivity)

  - Definition: Recall is the ratio of correctly predicted positive observations to all observations in the actual class. In fraud detection, it answers the question, "Of all the fraudulent transactions, how many did the model successfully identify?"
  - Formula: Recall = TP / (TP + FN), where TP is True Positives and FN is False Negatives.
  - Importance: High recall is crucial in fraud detection because it indicates a low rate of false negatives (fraudulent transactions that are not detected). Missing fraudulent transactions can lead to significant financial losses and security breaches.

Balancing Precision and Recall

In credit card fraud detection:

  - High Precision: Means fewer legitimate transactions are incorrectly flagged as fraudulent, reducing inconvenience to customers. However, this might come at the cost of missing some fraudulent transactions.
  - High Recall: Ensures that most fraudulent transactions are caught, enhancing security. However, this might result in more legitimate transactions being flagged as fraudulent, potentially leading to customer dissatisfaction.

Balancing these two metrics is crucial. A model that excessively flags transactions as fraudulent (high recall but low precision) could annoy customers, while a model that is too conservative (high precision but low recall) could miss detecting actual fraud, leading to financial losses. The ideal balance depends on the costs associated with false positives and false negatives in the specific context of the application.

### Discussion of Results
<div class="col-sm mt-3 mt-md-0" style="float: left; margin-right: 20px;" >
    {% include figure.html path="assets/img/summary.jpg" title="example image" class="img-fluid rounded z-depth-1" %}
</div>

**Model Performance:**
- The **Random Forest** classifier emerges as the optimal model amongst those evaluated. It achieves a balance between precision and recall, both approximately 0.8, at the chosen threshold of 0.14. This balance is crucial in credit card fraud detection where it is important to correctly identify as many fraudulent transactions as possible (high recall) while minimizing the number of legitimate transactions incorrectly flagged as fraudulent (high precision).
- **Gradient Boosting** shows a high recall but very low precision, which indicates it is identifying most of the fraudulent transactions but at the cost of a high false positive rate. This could lead to a significant number of legitimate transactions being flagged as fraud, which may not be desirable.
- **KNN** and **Logistic Regression** models show extreme cases with KNN predicting most samples as the positive class (recall of 1.0 but precision of 0.0) and Logistic Regression predicting most samples as the negative class (recall of 0.0). These models appear to be ineffective for this particular problem.

**F1 Score:**
- The F1 score, which is the harmonic mean of precision and recall, is highest for the Random Forest model (0.72), corroborating its superior performance in balancing the precision-recall trade-off.

**Training Time:**
- In terms of training efficiency, the Random Forest model has a comparatively low training time, which is an important consideration when models need to be retrained frequently as new data becomes available.

From the models compared, the Random Forest classifier is recommended for the credit card fraud detection task. It not only provides the best balance between precision and recall but also has an excellent ability to differentiate between classes as evidenced by its ROC AUC score. Its relatively short training time suggests that it would be practical for operational use where models may need to be updated regularly.

For deployment, it's important to consider the operational environment and how the model's predictions will be acted upon. False positives, while lower than other models, will still occur and processes need to be in place to handle these efficiently. Likewise, while the recall is high, it is not 100%, so some fraudulent transactions may still go undetected.

Finally, ongoing monitoring of the model's performance is crucial, as changes in customer behavior or fraud tactics could change the model's effectiveness over time. Regular reevaluation and updates to the model, along with continuous improvements to feature engineering and threshold

adjustment, will be key to maintaining high performance in the face of evolving patterns of credit card use and fraud.

### Future Improvements

Some ways to improve the model...

1. **Try Anomaly Detection Techniques**:
   - Since fraud is relatively rare, it can be treated as an anomaly. Anomaly detection algorithms could potentially be more effective in identifying fraud cases.

2. **Ensemble Methods**:
   - Use ensemble methods like Random Forest or Gradient Boosting that aggregate the decisions of multiple models to improve performance. Bagging and boosting can help reduce variance and bias, respectively.

3. **Use Advanced Algorithms**:
   - Explore advanced algorithms that are specifically designed for imbalanced data, such as SMOTE for oversampling or anomaly detection algorithms like Isolation Forest or One-Class SVM.

4. **Tune Hyperparameters**:
    - Optimize the model hyperparameters with larger parameter space.

