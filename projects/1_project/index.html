<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Fraud detection model pipeline | Hye Won (Nicole) Hwang</title> <meta name="author" content="Hye Won (Nicole) Hwang"> <meta name="description" content="Building a Machine Learning Pipeline for Fraud Detection"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hyeniii.github.io/projects/1_project/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Hye Won (Nicole) Hwang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Fraud detection model pipeline</h1> <p class="post-description">Building a Machine Learning Pipeline for Fraud Detection</p> </header> <article> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/12-480.webp 480w, /assets/img/12-800.webp 800w, /assets/img/12-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="credit card fraud image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h2 id="introduction">Introduction</h2> <p>This project develops a flexible machine learning pipeline tailored for detecting credit card fraud. Its core feature is the ability to train various models through configurable YAML files, allowing users to test different approaches easily. Integrated with MLflow, the pipeline tracks and manages training experiments, aiding in the comparison and evaluation of diverse models.</p> <p>A significant aspect of this pipeline is its capability to handle imbalanced data, a common challenge in fraud detection. Users can experiment with different sampling methods to balance the dataset, ensuring more accurate and reliable model performance. This flexibility, combined with comprehensive experiment tracking, positions the pipeline as a versatile tool in the fight against credit card fraud.</p> <h2 id="project-setup">Project setup</h2> <p>The project uses a variety of tools and libraries, including</p> <ul> <li>pandas for data manipulation,</li> <li>scikit-learn for machine learning,</li> <li>MLflow for experiment tracking,</li> <li>joblib for model serialization</li> </ul> <p>The configuration is managed using YAML files, offering a flexible and dynamic way to handle various parameters.</p> <p>Here is an example of the config yaml file…</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">experiment_name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">RandomForestClassifier</span><span class="nv"> </span><span class="s">hp</span><span class="nv"> </span><span class="s">tuning"</span>
<span class="na">data_path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">data/creditcard.csv"</span>
<span class="na">target</span><span class="pi">:</span> <span class="s2">"</span><span class="s">Class"</span>

<span class="na">model</span><span class="pi">:</span>
  <span class="na">class</span><span class="pi">:</span> <span class="s2">"</span><span class="s">RandomForestClassifier"</span>
  <span class="na">module</span><span class="pi">:</span> <span class="s2">"</span><span class="s">sklearn.ensemble"</span>
  <span class="na">params</span><span class="pi">:</span>
    <span class="na">n_estimators</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">100</span><span class="pi">,</span> <span class="nv">500</span><span class="pi">]</span>
    <span class="na">max_depth</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">10</span><span class="pi">,</span> <span class="nv">50</span><span class="pi">]</span>
  <span class="na">save_path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">artifacts/model/rf_model.joblib"</span>

<span class="na">preprocessing</span><span class="pi">:</span>
  <span class="na">sampler</span><span class="pi">:</span>
    <span class="na">class</span><span class="pi">:</span> <span class="s2">"</span><span class="s">RandomOverSampler"</span>
    <span class="na">module</span><span class="pi">:</span> <span class="s2">"</span><span class="s">imblearn.over_sampling"</span>
  <span class="na">scaler</span><span class="pi">:</span>
    <span class="na">class</span><span class="pi">:</span> <span class="s2">"</span><span class="s">StandardScaler"</span>
    <span class="na">module</span><span class="pi">:</span> <span class="s2">"</span><span class="s">sklearn.preprocessing"</span>
</code></pre></div></div> <p>Experimenter can simply change preprocessing and modeling methods by changing the corresponding methods in the config file and run the experiment.</p> <h3 id="imbalance-in-dataset-strategies-and-considerations">Imbalance in Dataset: Strategies and Considerations</h3> <div class="col-sm-5 mt-3 mt-md-0" style="float: left; margin-right: 20px;"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/imbalance-480.webp 480w, /assets/img/imbalance-800.webp 800w, /assets/img/imbalance-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/imbalance.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>In machine learning, especially in problems like credit card fraud detection, class imbalance poses a significant challenge. The imbalance occurs when one class (e.g., fraudulent transactions) is vastly outnumbered by another (e.g., legitimate transactions). This imbalance can lead to models that are biased towards the majority class, often at the expense of poorly predicting the minority class, which is usually the class of interest.</p> <h5 id="strategies-for-handling-imbalance">Strategies for Handling Imbalance</h5> <ul> <li> <p><strong>Class Weights</strong>: For models that support the class_weight parameter (such as many in scikit-learn), assigning a higher weight to the minority class helps counterbalance its underrepresentation. In our pipeline, we’ve integrated <code class="language-plaintext highlighter-rouge">class_weight='balanced'</code> for such models. This approach effectively tells the model to “pay more attention” to the minority class, which is crucial in fraud detection where failing to detect fraud (a false negative) can be very costly.</p> </li> <li> <p><strong>Sampling Methods</strong>: When class weights are not an option, or to explore different approaches, we can use sampling methods. These methods adjust the dataset before training the model. We’ve provided options for:</p> <ul> <li> <strong>Undersampling</strong>: Reducing the number of instances in the majority class.</li> <li> <strong>Oversampling</strong>: Increasing the number of instances in the minority class.</li> <li> <strong>SMOTE (Synthetic Minority Over-sampling Technique)</strong>: Creating synthetic instances of the minority class.</li> </ul> </li> </ul> <h5 id="the-debate-around-smote">The Debate Around SMOTE</h5> <p>While SMOTE is a popular and powerful method to handle imbalanced datasets, it’s not without its critics. Some concerns include:</p> <ul> <li> <p><strong>Distorted Data Space</strong>: Synthetic points generated by SMOTE may not represent the true data distribution, potentially leading the model to learn incorrect patterns.</p> </li> <li> <p><strong>Noise</strong>: If the minority class has outliers, SMOTE may amplify these outliers by creating more synthetic instances around them.</p> </li> </ul> <h3 id="results-and-comparisons">Results and Comparisons</h3> <h4 id="precision-recall-curves">Precision-Recall Curves</h4> <div class="col-sm-8 mt-3 mt-md-0" style="float: left; margin-right: 20px;"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/precision-recall-curve-480.webp 480w, /assets/img/precision-recall-curve-800.webp 800w, /assets/img/precision-recall-curve-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/precision-recall-curve.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>I compared the model performances using precision-recall curve. ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.</p> <h5 id="roc-curves-and-balanced-datasets">ROC Curves and Balanced Datasets</h5> <ul> <li>ROC Curve Basics: <ul> <li>The ROC curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.</li> <li>TPR (also known as recall) is calculated as TP / (TP + FN) and FPR is FP / (FP + TN), where TP, FP, FN, and TN represent true positives, false positives, false negatives, and true negatives, respectively.</li> </ul> </li> <li>Effect of Balanced Data: <ul> <li>In a balanced dataset, where the number of instances in each class is roughly equal, both the TPR and FPR are affected equally by the class distribution.</li> <li>This balance ensures that the ROC curve provides a reliable indication of how well the classifier can distinguish between the two classes under different thresholds.</li> </ul> </li> </ul> <h5 id="precision-recall-curves-and-imbalanced-datasets">Precision-Recall Curves and Imbalanced Datasets</h5> <ul> <li>Precision-Recall Curve Basics: <ul> <li>The precision-recall curve plots precision (TP / (TP + FP)) against recall (TPR). Unlike the ROC curve, the precision-recall curve focuses on the performance of the classifier only on the positive class.</li> </ul> </li> <li>Effect of Imbalanced Data: <ul> <li>In imbalanced datasets, especially when the positive class (minority class) is much smaller than the negative class, ROC curves can be misleadingly optimistic. This is because the FPR does not capture the model’s ability to correctly identify the minority class—it can have a low FPR by simply classifying most instances as the majority class.</li> <li>Precision, however, directly reflects the model’s ability to correctly identify positive instances among all instances it labeled as positive. In imbalanced scenarios, this is a more crucial measure.</li> <li>The precision-recall curve is more informative in these situations as it illustrates the trade-off between precision and recall (sensitivity to the minority class) without being affected by the large number of true negatives, which can dominate the FPR measure.</li> </ul> </li> </ul> <p>Precision</p> <ul> <li>Definition: Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. In the context of fraud detection, it answers the question, “Of all transactions classified as fraudulent, how many are actually fraudulent?”</li> <li>Formula: Precision = TP / (TP + FP), where TP is True Positives and FP is False Positives.</li> <li>Importance: High precision indicates a low rate of false positives (legitimate transactions incorrectly labeled as fraud). This is important in fraud detection because false positives can lead to customer dissatisfaction and unnecessary verification procedures.</li> </ul> <p>Recall (Sensitivity)</p> <ul> <li>Definition: Recall is the ratio of correctly predicted positive observations to all observations in the actual class. In fraud detection, it answers the question, “Of all the fraudulent transactions, how many did the model successfully identify?”</li> <li>Formula: Recall = TP / (TP + FN), where TP is True Positives and FN is False Negatives.</li> <li>Importance: High recall is crucial in fraud detection because it indicates a low rate of false negatives (fraudulent transactions that are not detected). Missing fraudulent transactions can lead to significant financial losses and security breaches.</li> </ul> <p>Balancing Precision and Recall</p> <p>In credit card fraud detection:</p> <ul> <li>High Precision: Means fewer legitimate transactions are incorrectly flagged as fraudulent, reducing inconvenience to customers. However, this might come at the cost of missing some fraudulent transactions.</li> <li>High Recall: Ensures that most fraudulent transactions are caught, enhancing security. However, this might result in more legitimate transactions being flagged as fraudulent, potentially leading to customer dissatisfaction.</li> </ul> <p>Balancing these two metrics is crucial. A model that excessively flags transactions as fraudulent (high recall but low precision) could annoy customers, while a model that is too conservative (high precision but low recall) could miss detecting actual fraud, leading to financial losses. The ideal balance depends on the costs associated with false positives and false negatives in the specific context of the application.</p> <h3 id="discussion-of-results">Discussion of Results</h3> <div class="col-sm mt-3 mt-md-0" style="float: left; margin-right: 20px;"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/summary-480.webp 480w, /assets/img/summary-800.webp 800w, /assets/img/summary-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/summary.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p><strong>Model Performance:</strong></p> <ul> <li>The <strong>Random Forest</strong> classifier emerges as the optimal model amongst those evaluated. It achieves a balance between precision and recall, both approximately 0.8, at the chosen threshold of 0.14. This balance is crucial in credit card fraud detection where it is important to correctly identify as many fraudulent transactions as possible (high recall) while minimizing the number of legitimate transactions incorrectly flagged as fraudulent (high precision).</li> <li> <strong>Gradient Boosting</strong> shows a high recall but very low precision, which indicates it is identifying most of the fraudulent transactions but at the cost of a high false positive rate. This could lead to a significant number of legitimate transactions being flagged as fraud, which may not be desirable.</li> <li> <strong>KNN</strong> and <strong>Logistic Regression</strong> models show extreme cases with KNN predicting most samples as the positive class (recall of 1.0 but precision of 0.0) and Logistic Regression predicting most samples as the negative class (recall of 0.0). These models appear to be ineffective for this particular problem.</li> </ul> <p><strong>F1 Score:</strong></p> <ul> <li>The F1 score, which is the harmonic mean of precision and recall, is highest for the Random Forest model (0.72), corroborating its superior performance in balancing the precision-recall trade-off.</li> </ul> <p><strong>Training Time:</strong></p> <ul> <li>In terms of training efficiency, the Random Forest model has a comparatively low training time, which is an important consideration when models need to be retrained frequently as new data becomes available.</li> </ul> <p>From the models compared, the Random Forest classifier is recommended for the credit card fraud detection task. It not only provides the best balance between precision and recall but also has an excellent ability to differentiate between classes as evidenced by its ROC AUC score. Its relatively short training time suggests that it would be practical for operational use where models may need to be updated regularly.</p> <p>For deployment, it’s important to consider the operational environment and how the model’s predictions will be acted upon. False positives, while lower than other models, will still occur and processes need to be in place to handle these efficiently. Likewise, while the recall is high, it is not 100%, so some fraudulent transactions may still go undetected.</p> <p>Finally, ongoing monitoring of the model’s performance is crucial, as changes in customer behavior or fraud tactics could change the model’s effectiveness over time. Regular reevaluation and updates to the model, along with continuous improvements to feature engineering and threshold</p> <p>adjustment, will be key to maintaining high performance in the face of evolving patterns of credit card use and fraud.</p> <h3 id="future-improvements">Future Improvements</h3> <p>Some ways to improve the model…</p> <ol> <li> <strong>Try Anomaly Detection Techniques</strong>: <ul> <li>Since fraud is relatively rare, it can be treated as an anomaly. Anomaly detection algorithms could potentially be more effective in identifying fraud cases.</li> </ul> </li> <li> <strong>Ensemble Methods</strong>: <ul> <li>Use ensemble methods like Random Forest or Gradient Boosting that aggregate the decisions of multiple models to improve performance. Bagging and boosting can help reduce variance and bias, respectively.</li> </ul> </li> <li> <strong>Use Advanced Algorithms</strong>: <ul> <li>Explore advanced algorithms that are specifically designed for imbalanced data, such as SMOTE for oversampling or anomaly detection algorithms like Isolation Forest or One-Class SVM.</li> </ul> </li> <li> <strong>Tune Hyperparameters</strong>: <ul> <li>Optimize the model hyperparameters with larger parameter space.</li> </ul> </li> </ol> </article> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Hye Won (Nicole) Hwang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>