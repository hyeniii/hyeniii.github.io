<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Transformer from scratch (Pytorch) | Hye Won (Nicole) Hwang</title> <meta name="author" content="Hye Won (Nicole) Hwang"> <meta name="description" content="My attempt to understand transformers by building it from scratch"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hyeniii.github.io/projects/7_project/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Hye Won (Nicole) Hwang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Transformer from scratch (Pytorch)</h1> <p class="post-description">My attempt to understand transformers by building it from scratch</p> </header> <article> <h1 id="building-transformers-from-scratch">Building Transformers from Scratch!</h1> <h2 id="what-are-transformers">What are transformers??</h2> <p>Transformers are deep learning models that are able to process sequential data. For example, transformers can process words, biological sequences, time series, etc…</p> <p>Transformers are considered more efficient than their sucessors (i.e. GRUs and LSTMs) because they can process al their unput in <strong>parallel</strong> while usual RNNs can only process information in <strong>one way</strong>. Also RNNs do not work well with long text documentation while transformers use attention to help draw connections between any part of the sequence.</p> <p>Attention Mechanism</p> <p><strong>Self Attention</strong></p> <ul> <li>adding some context to the words in a sentence</li> </ul> <div class="col-sm-8 mt-3 mt-md-0" style="float: left; margin-right: 20px;"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://miro.medium.com/v2/resize:fit:720/format:webp/1*XC-DMS9v98IhKLg8X5M0fA-480.webp 480w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*XC-DMS9v98IhKLg8X5M0fA-800.webp 800w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*XC-DMS9v98IhKLg8X5M0fA-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*XC-DMS9v98IhKLg8X5M0fA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <ul> <li> <p><em>Bark is very cute and he is a dog</em>. This sentence shows that proximity of words are not always relevant but context is, since he is more related to <code class="language-plaintext highlighter-rouge">Bark</code> and <code class="language-plaintext highlighter-rouge">dog</code> rather than <code class="language-plaintext highlighter-rouge">and</code> and <code class="language-plaintext highlighter-rouge">is</code>.</p> </li> <li> <p>Idea is to apply some weight to the word embedding to obtain final word embeeding with more context than the initial embedding.</p> </li> </ul> \[\begin{flalign*} &amp;V_{1}V_{1} = W_{11} \quad \quad \quad \quad \quad \quad W_{11} \\ &amp;V_{1}V_{2} = W_{12} \quad \text{normalize} \quad W_{12} \\ &amp;V_{1}V_{3} = W_{13} \quad \quad \longrightarrow \quad \quad W_{13} \\ &amp;\quad \quad \vdots \quad \quad \quad \quad \quad \quad \quad \quad \quad \vdots \\ &amp;V_{1}V_{9} = W_{19} \quad \quad \quad \quad \quad \quad W_{19} \end{flalign*}\] <p>find weights by multiplying (dot product) the initial embedding of the first word with tthe embedding of all other words in the sentence then normalized (to sum 1)</p> \[\begin{flalign*} &amp;W_{11}V_{1} + W_{12}V_{2} + ... W_{19}V_{9} = Y_{1} \\ &amp;W_{21}V_{1} + W_{22}V_{2} + ... W_{29}V_{9} = Y_{2} \\ &amp;... \\ &amp;W_{91}V_{1} + W_{92}V_{2} + ... W_{99}V_{9} = Y_{9} \\ \end{flalign*}\] <p>these weights are multiplied with the initial embeddings of all the words in the sentence. W11 to W19 are all weights that have the context of the first word V1. So when we are multiplying these weights to each word, we are essentially reweighing all the other words towards the first word.</p> <ul> <li>no weights are trained in this process</li> </ul> <p><strong>Query, Key, and Values</strong> In self-attention nothing is being trained so we replace the word embeddings (V, which occurs 3 times 2 in dot product and 1 in multipying again to weights) by query, key and values.</p> <p>Let’s say we want to make all the words similar with respect to the first word V1. We then send V1 as the Query word. This query word will then do a dot product with all the words in the sentence (V1 to V9) — and these are the Keys. So the combination of the Query and the Keys give us the weights. These weights are then multiplied with all the words again (V1 to V9) which act as Values. There we have it, the Query, Keys, and the Values. If you still have some doubts, figure 5 should be able to clear them.</p> <div class="col-sm-8 mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://miro.medium.com/v2/resize:fit:720/format:webp/1*be6TGe97KozFe3YeZdi8AQ-480.webp 480w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*be6TGe97KozFe3YeZdi8AQ-800.webp 800w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*be6TGe97KozFe3YeZdi8AQ-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*be6TGe97KozFe3YeZdi8AQ.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p><strong>Keys (K), Queries (Q), and Values (V)</strong>: Think of them as different representations of the input data. Each word (or token) in the input sequence is transformed into three different vectors – one each for key, query, and value.</p> <p><strong>Matrices Mk, Mq, and Mv</strong>: These are learnable weights in the network. When you multiply the key, query, and value vectors by these matrices, you’re essentially transforming them in a way that the network can learn to pay “attention” to certain parts of the input data more than others.</p> <p>database analogy:</p> <ol> <li> <p>Query in Database: You have a query (Q), and you want to find the most relevant information in the database. In the transformer, the query vector represents this.</p> </li> <li> <p>Finding the Key: The database (or the transformer model) compares your query with different keys (K) to find the most relevant one. In transformers, this is done using a dot product between Q and K vectors.</p> </li> <li> <p>Retrieving the Value: Once the relevant key is identified, the database gives you the corresponding value. In transformers, the value (V) associated with the most relevant key is focused on more.</p> </li> </ol> <p>\(attention(q,k,v) = \sum_{i} similarity(q,k_{i}) * v_{i}\)</p> <ol> <li>Calculate similarity measure using query and key (usually a dot product, scaled dot product etc…)</li> <li>Find weights \(a_{i}\) using <code class="language-plaintext highlighter-rouge">Softmax</code> </li> <li>Weighted combination of the results of the softmax (a) with the corresponding values (V). $$ \text{attention value} = \sum_{i} a_{i}V_{i}</li> </ol> <div class="col-sm-8 mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://miro.medium.com/v2/resize:fit:720/format:webp/1*iBtLFJu7eiGy5vhmOw56-w-480.webp 480w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*iBtLFJu7eiGy5vhmOw56-w-800.webp 800w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*iBtLFJu7eiGy5vhmOw56-w-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*iBtLFJu7eiGy5vhmOw56-w.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p><strong>Multi-Head Attention</strong></p> </article> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Hye Won (Nicole) Hwang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>