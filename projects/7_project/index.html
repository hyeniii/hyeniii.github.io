<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Transformer from scratch (Pytorch) | Hye Won (Nicole) Hwang</title> <meta name="author" content="Hye Won (Nicole) Hwang"> <meta name="description" content="My attempt to understand transformers by building it from scratch"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hyeniii.github.io/projects/7_project/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Hye Won (Nicole) Hwang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Transformer from scratch (Pytorch)</h1> <p class="post-description">My attempt to understand transformers by building it from scratch</p> </header> <article> <h1 id="building-transformers-from-scratch">Building Transformers from Scratch!</h1> <h2 id="what-are-transformers">What are transformers??</h2> <p>Transformers are deep learning models that are able to process sequential data. For example, transformers can process words, biological sequences, time series, etc…</p> <p>Transformers are considered more efficient than their sucessors (i.e. GRUs and LSTMs) because they can process al their unput in <strong>parallel</strong> while usual RNNs can only process information in <strong>one way</strong>. Also RNNs do not work well with long text documentation while transformers use attention to help draw connections between any part of the sequence.</p> <p>In order to understand Transformers we first need a good understanding of the Attention Mechanism</p> <h2 id="self-attention"><strong>Self Attention</strong></h2> <ul> <li>adding some context to the words in a sentence</li> </ul> <div class="col-sm-8 mt-3 mt-md-0" style="float: left; margin-right: 20px;"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://miro.medium.com/v2/resize:fit:720/format:webp/1*XC-DMS9v98IhKLg8X5M0fA-480.webp 480w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*XC-DMS9v98IhKLg8X5M0fA-800.webp 800w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*XC-DMS9v98IhKLg8X5M0fA-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*XC-DMS9v98IhKLg8X5M0fA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <ul> <li> <p><em>Bark is very cute and he is a dog</em>. This sentence shows that proximity of words are not always relevant but context is, since he is more related to <code class="language-plaintext highlighter-rouge">Bark</code> and <code class="language-plaintext highlighter-rouge">dog</code> rather than <code class="language-plaintext highlighter-rouge">and</code> and <code class="language-plaintext highlighter-rouge">is</code>.</p> </li> <li> <p>Idea is to apply some weight to the word embedding to obtain final word embeeding with more context than the initial embedding.</p> </li> </ul> \[\begin{flalign*} &amp;V_{1}V_{1} = W_{11} \quad \quad \quad \quad \quad \quad W_{11} \\ &amp;V_{1}V_{2} = W_{12} \quad \text{normalize} \quad W_{12} \\ &amp;V_{1}V_{3} = W_{13} \quad \quad \longrightarrow \quad \quad W_{13} \\ &amp;\quad \quad \vdots \quad \quad \quad \quad \quad \quad \quad \quad \quad \vdots \\ &amp;V_{1}V_{9} = W_{19} \quad \quad \quad \quad \quad \quad W_{19} \end{flalign*}\] <p>find weights by multiplying (dot product) the initial embedding of the first word with tthe embedding of all other words in the sentence then normalized (to sum 1)</p> \[\begin{flalign*} &amp;W_{11}V_{1} + W_{12}V_{2} + ... W_{19}V_{9} = Y_{1} \\ &amp;W_{21}V_{1} + W_{22}V_{2} + ... W_{29}V_{9} = Y_{2} \\ &amp;... \\ &amp;W_{91}V_{1} + W_{92}V_{2} + ... W_{99}V_{9} = Y_{9} \\ \end{flalign*}\] <p>these weights are multiplied with the initial embeddings of all the words in the sentence. W11 to W19 are all weights that have the context of the first word V1. So when we are multiplying these weights to each word, we are essentially reweighing all the other words towards the first word.</p> <ul> <li>no weights are trained in this process</li> </ul> <h5 id="query-key-and-values"><strong>Query, Key, and Values</strong></h5> <p>In self-attention nothing is being trained so we replace the word embeddings (V, which occurs 3 times 2 in dot product and 1 in multipying again to weights) by query, key and values.</p> <p>Let’s say we want to make all the words similar with respect to the first word V1. We then send V1 as the Query word. This query word will then do a dot product with all the words in the sentence (V1 to V9) — and these are the Keys. So the combination of the Query and the Keys give us the weights. These weights are then multiplied with all the words again (V1 to V9) which act as Values. There we have it, the Query, Keys, and the Values. If you still have some doubts, figure 5 should be able to clear them.</p> <div class="col-sm-10 mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://miro.medium.com/v2/resize:fit:720/format:webp/1*be6TGe97KozFe3YeZdi8AQ-480.webp 480w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*be6TGe97KozFe3YeZdi8AQ-800.webp 800w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*be6TGe97KozFe3YeZdi8AQ-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*be6TGe97KozFe3YeZdi8AQ.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p><strong>Keys (K), Queries (Q), and Values (V)</strong>: Think of them as different representations of the input data. Each word (or token) in the input sequence is transformed into three different vectors – one each for key, query, and value.</p> <p><strong>Matrices Mk, Mq, and Mv</strong>: These are learnable weights in the network. When you multiply the key, query, and value vectors by these matrices, you’re essentially transforming them in a way that the network can learn to pay “attention” to certain parts of the input data more than others.</p> <p>database analogy:</p> <ol> <li> <p>Query in Database: You have a query (Q), and you want to find the most relevant information in the database. In the transformer, the query vector represents this.</p> </li> <li> <p>Finding the Key: The database (or the transformer model) compares your query with different keys (K) to find the most relevant one. In transformers, this is done using a dot product between Q and K vectors.</p> </li> <li> <p>Retrieving the Value: Once the relevant key is identified, the database gives you the corresponding value. In transformers, the value (V) associated with the most relevant key is focused on more.</p> </li> </ol> <p>\(attention(q,k,v) = \sum_{i} similarity(q,k_{i}) * v_{i}\)</p> <ol> <li>Calculate similarity measure using query and key (usually a dot product, scaled dot product etc…)</li> <li>Find weights \(a_{i}\) using <code class="language-plaintext highlighter-rouge">Softmax</code> </li> <li>Weighted combination of the results of the softmax (a) with the corresponding values (V). \(\text{attention value} = \sum_{i} a_{i}V_{i}\)</li> </ol> <div class="col-sm-10 mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://miro.medium.com/v2/resize:fit:720/format:webp/1*iBtLFJu7eiGy5vhmOw56-w-480.webp 480w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*iBtLFJu7eiGy5vhmOw56-w-800.webp 800w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*iBtLFJu7eiGy5vhmOw56-w-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*iBtLFJu7eiGy5vhmOw56-w.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h5 id="multi-head-attention"><strong>Multi-Head Attention</strong></h5> <p><em>Bark is very cute and he is a dog</em>. Taking the word ‘dog’, the words ‘Bark’, ‘cute’, ‘he’ has some significance/ relavance to the word. From this sentence we can understand that the dog’s name is Bark. It is male and cute. Just one attention mechanism may not be able to identify all three words relevant to the word ‘dog’. Therefore we can add more attentions to better signify the words related to the target word. This reduces the load on one attention to find all significant words and increases the chances of finding more relevant words.</p> <p>When we add more linear layers as keys, queries, and values, these are able to be trained in parallel and have independent weights. The three attention blocks are concatenated at the end to give one final attention output</p> <div class="col-sm-10 mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://miro.medium.com/v2/resize:fit:720/format:webp/1*kjDdb-8rnsJzru0Cj5BEVA-480.webp 480w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*kjDdb-8rnsJzru0Cj5BEVA-800.webp 800w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*kjDdb-8rnsJzru0Cj5BEVA-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*kjDdb-8rnsJzru0Cj5BEVA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <blockquote> Multi-head attention in a Transformer processes a sentence by simultaneously applying multiple attention mechanisms to capture different contextual relationships between words, thereby enriching the sentence representation with diverse perspectives of relevance and interaction. In the context of the sentence 'the cat sat on the mat', multi-head attention in a Transformer model aims to analyze and understand the sentence from multiple perspectives, allowing the model to capture various aspects of how each word relates to others in the sentence, such as the relationship between 'cat' and 'sat', or 'mat' and 'on', thereby creating a richer, more context-aware representation of the entire sentence. </blockquote> <p>so how do these blocks of attention help create the Transfromer network??</p> <h2 id="transformer-network">Transformer Network</h2> <div class="col-sm-10 mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://miro.medium.com/v2/resize:fit:640/format:webp/1*9XuOogviDS6hkWGL2qIKQA-480.webp 480w, https://miro.medium.com/v2/resize:fit:640/format:webp/1*9XuOogviDS6hkWGL2qIKQA-800.webp 800w, https://miro.medium.com/v2/resize:fit:640/format:webp/1*9XuOogviDS6hkWGL2qIKQA-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*9XuOogviDS6hkWGL2qIKQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>The transformer network contains 2 parts: Encoder and Decoder.</p> <p>In machine translation, encoder is used to encode the initial sentence and decoder is used to produce a translated sentence.</p> <h3 id="encoder">Encoder</h3> <ol> <li> <strong>Input</strong>: sentence if fed at once</li> <li> <strong>Input Embedding</strong>: Words in sentence is located and represented from the pre-trained embedding space.</li> <li> <strong>Positional Embedding</strong>: words in different sentences (i.e. the cat sat on the mat vs the mat sat on the cat)can have different meaning so we need positional embedding to give information based on the context and position of the word in a sentence. Transformers do not load the sentence sequentially (loads in parallel), therefore we need to explicitly define the position of the words in a sentence. ex.</li> </ol> <h5 id="positional-embedding">Positional Embedding</h5> <p>we add positional embedding to each word:</p> <ul> <li>“The” (word embedding) + Position 1 (positional embedding)</li> <li>“cat” (word embedding) + Position 2 (positional embedding)</li> <li>“sat” (word embedding) + Position 3 (positional embedding)</li> <li>“on” (word embedding) + Position 4 (positional embedding)</li> <li>“the” (word embedding) + Position 5 (positional embedding)</li> <li>“mat” (word embedding) + Position 6 (positional embedding)</li> </ul> <p>So, even if the word “the” appears twice, its overall representation will be different each time due to the addition of different positional embeddings.</p> <p>Common way to calculate positional embedding is sine/cosine functions (not optimal for image data).</p> <p>The formulae for positional encoding at position <code class="language-plaintext highlighter-rouge">pos</code> and for dimension <code class="language-plaintext highlighter-rouge">i</code> are:</p> <ul> <li> \[PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})\] </li> <li> \[PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})\] </li> </ul> <p>where <code class="language-plaintext highlighter-rouge">d_model</code> is the size of the embedding (4 in our simplified example) and <code class="language-plaintext highlighter-rouge">i</code> is the dimension.</p> <p>For each position in the sentence, we calculate the positional embedding vector. Let’s do this for the first couple of positions in each sentence.</p> <ul> <li>Position 1 (for “The”): <ul> <li> \[PE_{(1, 0)} = \sin(1 / 10000^{0/4}) = \sin(1)\] </li> <li> \[PE_{(1, 1)} = \cos(1 / 10000^{0/4}) = \cos(1)\] </li> <li> \[PE_{(1, 2)} = \sin(1 / 10000^{2/4})\] </li> <li> \[PE_{(1, 3)} = \cos(1 / 10000^{2/4})\] </li> </ul> </li> <li>Position 2 (for “cat” in the first sentence and “mat” in the second): <ul> <li> \[PE_{(2, 0)} = \sin(2 / 10000^{0/4})\] </li> <li> \[PE_{(2, 1)} = \cos(2 / 10000^{0/4})\] </li> <li> \[PE_{(2, 2)} = \sin(2 / 10000^{2/4})\] </li> <li> \[PE_{(2, 3)} = \cos(2 / 10000^{2/4})\] </li> </ul> </li> <li>Each position in a sentence gets a unique positional embedding vector.</li> <li>These embeddings do not encode semantic similarity (like “cat” being closer to “sat” than “mat”). Instead, they encode positional information.</li> <li>For both sentences, the embedding for “The” (position 1) will be the same. However, the embedding for “cat” (position 2 in the first sentence) and “mat” (position 2 in the second sentence) will also be the same, even though they are different words, because they are at the same position in their respective sentences.</li> </ul> <ol> <li> <strong>Multi-Head Attention</strong>: The final embedding flows into the multi-head attention where the block receives a vector (sentence) that contains subvectors (words in a sentence). The multi-head attention then computes the attention between every position with every other postiion of the vector.</li> </ol> <p>The idea of multi-head attention is to take a word embedding, combine it with some other word embedding (or multiple words) using attention (or multiple attentions) to produce a better embedding for that word (embedding with a lot more context of the surrounding words).</p> <ol> <li> <strong>Add &amp; Norm and Feed-Forward</strong>:</li> </ol> <p>The next block is the ‘Add &amp; Norm’ which takes in a residual connection of the original word embedding, adds it to the embedding from the multi-head attention, and then normalizes it to have zero mean and variance 1.</p> <p>This is fed to a ‘feed forward’ block which also has an ‘add &amp; norm’ block at its output.</p> <p>The whole multi-head attention and feed-forward blocks are repeated n times (hyperparameters), in the encoder block.</p> <blockquote> The output of the encoder is again a sequence of embeddings, one embedding per position, where each position embedding contains not only the embedding of the original word at the position but also information about other words, that it learned using attention. </blockquote> <h3 id="decoder">Decoder</h3> <ol> <li> <p>In sentence translation, the decoder block takes in the French sentence (for English to French translation). Like the encoder, here we add a word embedding and a positional embedding and feed it to the multi-head attention block.</p> </li> <li> <p>The self-attention block will generate an attention vector for each word in the French sentence, to show how much one word is related to the other in the sentence.</p> </li> <li> <p>This attention vector from the French sentence is then compared with the attention vectors from the English sentence. This is the part where the English to French word mappings happen.</p> </li> <li> <p>In the final layers, the decoder predicts the translation of the English word to the best probable French word.</p> </li> <li> <p>The whole process is repeated multiple times to get a translation of the entire text data.</p> </li> </ol> <div class="col-sm-10 mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://miro.medium.com/v2/resize:fit:720/format:webp/1*ag-93N1KFg67-qOjBo9Unw-480.webp 480w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*ag-93N1KFg67-qOjBo9Unw-800.webp 800w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*ag-93N1KFg67-qOjBo9Unw-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*ag-93N1KFg67-qOjBo9Unw.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h5 id="masked-multi-head-attention">Masked Multi-head Attention</h5> <p>There is one block that is new in the decoder — the Masked Multi-head Attention. All the other blocks, we have already seen previously in the encoder.</p> <p>This is a multi-head attention block where some values are masked. The probabilities of the masked values are nullified or not selected.</p> <p>For example, while decoding, the output value should only depend on previous outputs and not future outputs. Then we mask the future outputs.</p> <h3 id="references">References</h3> <p>https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021</p> <p>https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada</p> <p>https://hyugen-ai.medium.com/transformers-in-pytorch-from-scratch-for-nlp-beginners-ff3b3d922ef7</p> </article> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Hye Won (Nicole) Hwang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>