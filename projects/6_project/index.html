<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Movie Recommender based on plot similarity | Hye Won (Nicole) Hwang</title> <meta name="author" content="Hye Won (Nicole) Hwang"> <meta name="description" content="use NLP (Natural Language Processing) and KMeans to predict the similarity between movies based on the plot from IMDB and Wikipedia"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hyeniii.github.io/projects/6_project/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Hye Won (Nicole) Hwang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Movie Recommender based on plot similarity</h1> <p class="post-description">use NLP (Natural Language Processing) and KMeans to predict the similarity between movies based on the plot from IMDB and Wikipedia</p> </header> <article> <h2 id="movie-recommender">Movie Recommender</h2> <p>Utilize tokenization, stemming and vectorize using TfidfVectorizer. Utilize KMeans to create clusters using vectorized data and cosine similarity to find movie plots that are similar to one another.</p> <p><a href="https://movie-recommender-v0wd.onrender.com/" rel="external nofollow noopener" target="_blank">Check out the product frontend!</a></p> <h3 id="data">Data</h3> <p><a href="https://www.kaggle.com/datasets/devendra45/movies-similarity" rel="external nofollow noopener" target="_blank">Source</a></p> <h3 id="tokenization-and-stemming">Tokenization and Stemming</h3> <p><strong>Tokenization</strong> is the process of breaking down text into smaller units called tokens, typically words or phrases. This is crucial for NLP as it transforms unstructured text into a format that’s easier to analyze.</p> <ul> <li>Word Tokenization: Splits the text into words. It’s the most common form of tokenization.</li> <li>Sentence Tokenization: Divides text into sentences. Useful in tasks where sentence-level analysis is required, like sentiment analysis.</li> <li>Subword Tokenization: Breaks words into smaller units (like syllables or morphemes). This is helpful in languages where compound words are common.</li> </ul> <p><strong>Stemming</strong> involves reducing words to their base or root form. For instance, “running”, “runs”, “ran” are all reduced to the stem “run”. This helps in standardizing words for better analysis.</p> <ul> <li>Porter Stemmer: A widely used, relatively gentle stemmer.</li> <li>Lancaster Stemmer: A more aggressive stemmer than Porter.</li> <li>Snowball Stemmer: An improvement over Porter, available for several languages.</li> </ul> <h3 id="vectorization-methods-tfidfvectorizer">Vectorization Methods (TfidfVectorizer)</h3> <p>After tokenization and stemming, the next step is <strong>vectorization</strong>. This converts text data into numerical values which can be processed by machine learning algorithms. <strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong> is a popular method. It reflects the importance of a word in a document in a corpus.</p> <ul> <li>Count Vectorization: Represents documents using the frequency of each word. It’s simple but can give undue weight to frequent words.</li> <li>TF-IDF (Term Frequency-Inverse Document Frequency): Reflects how important a word is to a document in a corpus. It reduces the impact of frequent words across documents.</li> <li>Word Embeddings (like Word2Vec, GloVe): These provide more sophisticated representations by capturing semantic meanings and relationships between words.</li> </ul> <p>Absolutely, let’s dive into the specifics of each text vectorization method and how they are calculated:</p> <h5 id="1-count-vectorization">1. Count Vectorization</h5> <p>Count Vectorization, represents text documents as vectors where each dimension corresponds to a unique word in the text corpus. The value in each dimension is the frequency of that word in the document.</p> <p><strong>Calculation:</strong></p> <ul> <li> <strong>Step 1:</strong> Create a vocabulary of all unique words across all documents in the corpus.</li> <li> <strong>Step 2:</strong> For each document, count the number of times each word appears and place that count in the corresponding dimension of the vector.</li> </ul> <p>For example, if your vocabulary is [“apple”, “banana”, “orange”], and your document is “apple banana apple,” the count vector would be [2, 1, 0], indicating “apple” appears twice, “banana” once, and “orange” not at all.</p> <h5 id="2-tf-idf-term-frequency-inverse-document-frequency">2. TF-IDF (Term Frequency-Inverse Document Frequency)</h5> <p>TF-IDF is a statistical measure used to evaluate the importance of a word in a document, which is part of a corpus. The intuition behind this is that words that appear frequently in a document but not across many documents are likely more significant (ex. the).</p> <p><strong>Calculation:</strong></p> <ul> <li> <strong>Term Frequency (TF):</strong> The frequency of a word in a document. <ul> <li> \[TF(word) = \frac{\text{Number of times word appears in a document}}{\text{Total number of words in the document}}\] </li> </ul> </li> <li> <strong>Inverse Document Frequency (IDF):</strong> Measures the importance of the word across the corpus. <ul> <li> \[IDF(word) = \log\left(\frac{\text{Total number of documents}}{\text{Number of documents containing the word}}\right)\] </li> </ul> </li> <li> <strong>TF-IDF:</strong> The product of TF and IDF scores of a word. <ul> <li> \[TF\text{-}IDF(word) = TF(word) \times IDF(word)\] </li> </ul> </li> </ul> <p>A high TF-IDF score for a word in a document indicates it’s important and relatively unique to that document.</p> <h5 id="3-word-embeddings-word2vec-glove">3. Word Embeddings (Word2Vec, GloVe)</h5> <p>Word Embeddings are techniques that represent words in a continuous vector space where semantically similar words are mapped to nearby points. They capture more nuanced relationships between words.</p> <p><strong>Calculation:</strong></p> <ul> <li> <strong>Word2Vec:</strong> Uses neural networks to learn word associations from a large corpus of text. There are two main architectures: <ul> <li> <strong>CBOW (Continuous Bag of Words):</strong> Predicts a word based on its context.</li> <li> <strong>Skip-gram:</strong> Predicts the context given a word.</li> </ul> </li> <li> <strong>GloVe (Global Vectors for Word Representation):</strong> Uses matrix factorization techniques on the co-occurrence matrix of words in the corpus. It essentially tries to minimize the difference between the dot product of the embeddings of two words and the logarithm of their co-occurrence probability.</li> </ul> <p>These methods involve complex mathematical operations and neural network training, often requiring large datasets and significant computational resources. The resulting vectors typically have hundreds of dimensions and capture subtle semantic relationships, unlike the simpler count-based methods.</p> <p>In summary, while Count Vectorization and TF-IDF provide a more basic form of text representation focusing on word frequencies, Word Embeddings offer a more nuanced and deeper understanding of word meanings and relationships.</p> <h3 id="clustering-using-kmeans">Clustering Using KMeans</h3> <p>Finally, you can use <strong>KMeans clustering</strong> to group similar movies based on their plot descriptions. KMeans is an unsupervised learning algorithm that partitions the data into K clusters.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/denogram-480.webp 480w, /assets/img/denogram-800.webp 800w, /assets/img/denogram-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/denogram.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h3 id="finding-similar-movies">Finding Similar Movies</h3> <p>To find movies similar to a given one, you can compute the cosine similarity between the TF-IDF vectors of the movies. Movies with higher cosine similarity scores are more similar in terms of plot.</p> <h3 id="conclusion">Conclusion</h3> <p>This approach leverages NLP and machine learning to group movies based on the similarity of their plots. W can further enhance this system by incorporating more features like genres, director, and cast. Remember, the quality of recommendations largely depends on the richness of your dataset and the fine-tuning of the model.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Hye Won (Nicole) Hwang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>